{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7665cda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6a71f8bc6d4956b8c3997c3133442f6",
     "grade": false,
     "grade_id": "cell-a6f327a43f356a64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Project 2: Predict the Next Frames\n",
    "\n",
    "Recurrent Neural Network is developed for processing variable length sequences of inputs, e.g., videos. Since you have already learned about RNN in the class, now you will apply it to an interesting task: predict the next frames of a video. Specifically, given the first half of a video, you will use RNN to predict the second half. \n",
    "\n",
    "Submission Instructions:\n",
    "Your submission file should be a type of zipfile that contains ``writeup.txt`` , directory `ckpt` that contains the visualization result images, and everything else you get after completing the coding section. \n",
    "\n",
    "```\n",
    "./project2/\n",
    "    |\n",
    "    |\n",
    "    |--------writeup.txt\n",
    "    |--------project2.ipynb\n",
    "    |--------ckpt/\n",
    "    |         |------[your visualization result images]\n",
    "    |\n",
    "    |--------README.txt\n",
    "    |--------images/\n",
    "    |--------project2_utils.py\n",
    "```\n",
    "\n",
    "<!-- Upload a ZIP archive with the completed jupyter notebook and a report to BlackBoard before the deadline.\n",
    "- Notebook: your implementation of RNN.\n",
    "- Report: everything else, containing the analysis of your code and qualitative/quantitative results. -->\n",
    "\n",
    "This project is due by **11:59pm EDT on October 31st 2021**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da24323",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4d0e1afd34862958d43007ca4e56309",
     "grade": false,
     "grade_id": "cell-27b14563a95afb42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Package\n",
    "\n",
    "Let's first import all the packages that you will need.\n",
    "\n",
    "- **torch, torch.nn, torch.nn.functional** are the fundamental modules in pytorch library, supporting Python programs that facilitates building deep learning projects.\n",
    "- **torchvision** is a library for Computer Vision that goes hand in hand with PyTorch\n",
    "- **numpy** is the fundamental package for scientific computing with Python programs.\n",
    "- **PIL, matplotlib** are libraries to plot graphs and save images in Python.\n",
    "- **os, random** are the standard modules in Python.\n",
    "- **argparse** is a library for writing user-friendly command-line interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07547d73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50eb78c1efb2c44da871e5a0adf9c91e",
     "grade": true,
     "grade_id": "cell-ef2268cfa3e8582d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import packages successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import random\n",
    "from project2_utils import *\n",
    "\n",
    "# Speicify which gpu to use\n",
    "gpu_ids = \"0\" # You can change it to other gpu id, e.g., \"1\" or \"2\" if working on a multi-gpu machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "\n",
    "print(\"Import packages successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa81427",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e0fefc249865e2d638d9a9ad54e477",
     "grade": false,
     "grade_id": "cell-8c4c3f58a8b7c83d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Dataset\n",
    "\n",
    "You will use a moving digits dataset for this project.\n",
    "\n",
    "Let's first use ``torch.utils.data.Dataset`` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce64e2ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dded04fc6e9652731dafa12dcec71726",
     "grade": true,
     "grade_id": "cell-42087e977ab1e41c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10000\n",
      "Number of testing examples: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the root directory of data \n",
    "root = '/u/cs298/project2/data'\n",
    "\n",
    "# load dataset\n",
    "train_set = MovingDigits(os.path.join(root, 'train.gz'), is_train=True)\n",
    "test_set = MovingDigits(os.path.join(root, 'test.pt'), is_train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=1,\n",
    "                 shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=1,\n",
    "                shuffle=False)\n",
    "\n",
    "print (\"Number of training examples: \" + str(train_set.__len__()))\n",
    "print (\"Number of testing examples: \" + str(test_set.__len__()))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "seq, seq_target = dataiter.next()\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = (npimg * 255).transpose(1,2,0).astype(np.uint8)\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.imshow(npimg, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# show images\n",
    "grid = torchvision.utils.make_grid(torch.cat((seq[0], seq_target[0]), 0), nrow=20, padding=0)\n",
    "imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f68dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab1737ea95185935ab2aa12c82e07071",
     "grade": false,
     "grade_id": "cell-e418102e76576911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Convolutional LSTM Cell\n",
    "\n",
    "In the class, you have learned the idea of of vanilla Long Short-Term Memory networks (LSTM), where the detailed equations are given as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma_g(W_{f} \\cdot [x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t &= \\sigma_g(W_{i} \\cdot [x_t, h_{t-1}] + b_i) \\\\\n",
    "c_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} \\cdot [x_t, h_{t-1}] + b_c) \\\\\n",
    "o_t &= \\sigma_g(W_{o} \\cdot [x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t &= o_t \\circ \\sigma_h(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "Compared to the LSTM Cell covered in the class, here you are required to implement a variant of it -- ConvLSTM. This variant modifies the inner workings of the LSTM mechanism by using the convolution operation instead of simple matrix multiplication (Note: original ConvLSTM cell design may use the cell state in all the gates, here we are actually implementing a variant of it). The equations are as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma_g(W_{f} * [x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t &= \\sigma_g(W_{i} * [x_t, h_{t-1}] + b_i) \\\\\n",
    "c_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} * [x_t, h_{t-1}] + b_c) \\\\\n",
    "o_t &= \\sigma_g(W_{o} * [x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t &= o_t \\circ \\sigma_h(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "where $*$ denotes convolutional operation and $\\circ$ denotes Hadamard product (element-wise product). The initial values are $c_0 = 0$ and $h_0 = 0$. The subscript $t$ indexes the time step. See the figure below as a reference: \n",
    "\n",
    "<img src=\"images/convlstm1.png\" width=\"60%\">\n",
    "\n",
    "**Variables** (omit the notation of batch size, height, width)\n",
    "\n",
    "* $x_t \\in \\mathbb{R}^{d}$: input tensor to the ConvLSTM unit\n",
    "* $f_t \\in {(0,1)}^{h}$: forget gate's activation tensor\n",
    "* $i_t \\in {(0,1)}^{h}$: input/update gate's activation tensor\n",
    "* $o_t \\in {(0,1)}^{h}$: output gate's activation tensor\n",
    "* $h_t \\in {(-1,1)}^{h}$: hidden state tensor also known as output tensor of the LSTM unit\n",
    "* $c_t \\in \\mathbb{R}^{h}$: cell state tensor\n",
    "* $W \\in \\mathbb{R}^{h \\times d}$, $ \\in \\mathbb{R}^{h \\times h} $ and $b \\in \\mathbb{R}^{h}$: weight matrices and bias tensor parameters which need to be learned during training\n",
    "\n",
    "where the superscripts $d$ and $h$ refer to the number of input features and number of hidden units, respectively.\n",
    "\n",
    "**Activation function**\n",
    "* $\\sigma_g$: sigmoid function.\n",
    "* $\\sigma_c$: hyperbolic tangent function.\n",
    "* $\\sigma_h$: hyperbolic tangent function.\n",
    "\n",
    "**Why ConvLSTM here?**\n",
    "For general-purpose sequence modeling, LSTM as a special RNN structure has proven stable and powerful for modeling long-range dependencies. In this kind of RNN structure, the previous hidden state is passed to the next step of sequence. Therefore, information on previous sequence data can be held and then used to predict the future. In our case of video prediction, LSTM is adopted to hold information on the past frames, and predict the future digit movements based on the previous observation.\n",
    "\n",
    "Although the vanilla LSTM cell has proven powerful for handling temporal correlation, it uses full connections in input-to-state and state-to-state transitions, in which no spatial information is encoded. To address this, we use a convolutional operator in the input-to-state and state-to-state transitions, such that the ConvLSTM cell can determine the future state of a certain cell in the grid by the inputs and past states of its local neighbors. With ConvLSTM, the motion of moving digits can be captured.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirement:** Given the above information, now you need to implement the ``ConvLSTM`` cell (``nn.lstm()`` is not allowed to call in your implementation):\n",
    "\n",
    "1.   Initialize the parameters in ConvLSTM Cell. You can call ``nn.Conv2d()`` to implment the convolution.\n",
    "2.   Given the current states, compute and update it to get the next states (based on the equations above).\n",
    "3.   Complete the state initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4840a784",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7033949cffbd6c5c71041c5e3e8f7b30",
     "grade": true,
     "grade_id": "cell-fb38fc30e49c55c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = None\n",
    "        self.hidden_dim = None\n",
    "\n",
    "        self.kernel_size = None\n",
    "        self.padding = None\n",
    "        self.bias = None\n",
    "        \n",
    "        self.conv = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 3, kernel_size[1] // 3\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input, cur_state): # given the input and current states, you need to get the next states\n",
    "        \"\"\"\n",
    "        Compute and update the states.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        input:\n",
    "            4-D Tensor of shape (b, c, h, w)        #   batch, channel, height, width\n",
    "        cur_state: \n",
    "            [h_current, c_current] input to current cell, including hidden state and cell input \n",
    "        \n",
    "        \n",
    "        ----------\n",
    "        Output: \n",
    "            [h_next, c_next] input to next cell, including update hidden state and cell output \n",
    "        \"\"\"     \n",
    "        h_cur = None\n",
    "        c_cur = None\n",
    "        \n",
    "        h_next = None\n",
    "        c_next = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        concat = torch.cat([input, h_cur], 1) \n",
    "        # chan\n",
    "        convolution = self.conv(concat)\n",
    "        var1, var2, var3, var4 = torch.split(convolution, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(var1)\n",
    "        f = torch.sigmoid(var2)\n",
    "        o = torch.sigmoid(var3)\n",
    "        g = torch.tanh(var4)\n",
    "        #print(self.hidden_dim)\n",
    "        #print(i)\n",
    "        #print(f)\n",
    "        #print(o)\n",
    "        #print(g)\n",
    "        #print(cc_f)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return  h_next,  c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        Initialize the input states.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Number of examples in a batch.\n",
    "        image_size: (int, int)\n",
    "            The spatial resolution of input images.\n",
    "            \n",
    "        ----------\n",
    "        Output: \n",
    "            [h_init, c_init] zero tensors\n",
    "        \n",
    "        Note: the initialized states should also be on the gpu machine\n",
    "        \"\"\"\n",
    "        h_init = None\n",
    "        c_init = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        height, width = image_size\n",
    "        \n",
    "        h_init=torch.rand(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        c_init=torch.rand(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return (h_init, c_init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9a0d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22886e053be89239e4153b2e8b1d1087",
     "grade": false,
     "grade_id": "cell-590ec83714d933f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Encoder-Decoder Sequnce-to-Sequence Architecture\n",
    "\n",
    "We follow the design patterns of RNNs that map a variable-length sequence to another variable-lenght sequence (in the class slides). An encoder that contains two ConvLSTM cells extract the \"context\" that represents the motion summary of the input sequential images and is given as input to the decoder RNN, which also contains two ConvLSTM cells. The network architecutre is as follows:\n",
    "\n",
    "<br/>\n",
    "\n",
    "Input ---> ConvLSTM 1 ---> ConvLSTM 2 ---> Encoded State ---> ConvLSTM 3 ---> ConvLSTM 4 ---> 3D CNN decoder ---> Output\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **Input:** In the dataset, the size of input image is a tensor $ X \\in \\mathbb{R}^{B \\times 1 \\times 64 \\times 64}$, where $B$ denotes the batch size. Generally, the input shape of a particular time step is ``[batch_size, num_channel, height, width]`` . ``Width`` and ``height`` are the image size, time step means which frame the network is processing.\n",
    "\n",
    "\n",
    "* **Encoder**\n",
    "  \n",
    "  * ConvLSTM Encoder 1: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` , where ``num_hidden_dim`` is the output dimension of parameters.\n",
    "  * ConvLSTM Encoder 2: the output shape is ``[batch_size, num_hidden_dim, height, width]`` .\n",
    "\n",
    "\n",
    "* **Decoder**\n",
    "  * ConvLSTM Decoder 3: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` .\n",
    "  * ConvLSTM Decoder 4: the output shape is ``[batch_size, num_hidden_dim,  height, width]`` .\n",
    "\n",
    "\n",
    "* **3D CNN Decoder:**\n",
    "Since we want to regress images from the hidden states, we need to transform the feature maps from decoder into actual predictions. \n",
    "To achieve this we implement a 3D-CNN layer: 1) Take as input ``[num_hidden_dim, height, width]`` for each example in batch and time step 2) Iterate over the $n$ hidden states 3) Output ``[num_channel, height, width]`` per iteration, which is the predicted frame.\n",
    "\n",
    "  * Sigmoid layer: For the activation layer, as we normalized images into [0,1] at the beginning, we use a sigmoid actvation function to transform the output values of 3D-CNN layer into [0,1] as well. \n",
    "\n",
    "**How to predict the next $n$ frames?**\n",
    "\n",
    "Rather than predict all future frames in one-go, you will feed the \"context\" obtained from encoder into ConvLSTM decoder to get a final hidden state at one time. This final hidden state will then go through the 3D CNN decoder to get an actual predicted frame. For the first predicted frame, the output from the encoder can be used as \"context\". For the $n$-th predicted frame ($n>1$), the final hidden state of ($n$-1)-th frame can be used as \"context\" to input to the ConvLSTM decoder. By iterating $n$ times, the network can predict the next $n$ frames.\n",
    "\n",
    "In this case, you can produce any number of predictions in the future without having to change the architecture.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirement:** Given the above information, you will need to build the encoder-decoder network:\n",
    "\n",
    "\n",
    "1.   Initialize the parameters in ``EncoderDecoderConvLSTM``. You can call ``nn.Conv3d()`` to implment the 3D CNN layer and use the above ConvLSTM cell to build the encoder and decoder.\n",
    "2.   Define the encoder->decoder forward pass in ``autoencder()``.\n",
    "3.   Complete the forward pass of the whole network: first initialize the states and then gradually update them. (Hint: you can call ``init_hidden()`` function in ConvLSTM cell for state initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb33786c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "821b917b4601da505a3bc2d35ecaabd1",
     "grade": true,
     "grade_id": "cell-073d7e45da0f13f8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, num_hidden_dim, in_channel):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoded Tensor (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoded Tensor as input\n",
    "        # Decoder (3D CNN) - regresses frames from hidden states\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_hidden_dim = None\n",
    "        self.in_channel = None\n",
    "        \n",
    "        self.encoder_1 = None\n",
    "        self.encoder_2 = None\n",
    "        self.decoder_3 = None\n",
    "        self.decoder_4 = None\n",
    "        \n",
    "        self.decoder_CNN = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.num_hidden_dim = None\n",
    "        self.in_channel = None\n",
    "        \n",
    "        self.encoder_1 = ConvLSTMCell(input_dim=in_channel,hidden_dim=num_hidden_dim,kernel_size=(3, 3),bias=False)\n",
    "        self.encoder_2 = ConvLSTMCell(input_dim=num_hidden_dim,hidden_dim=num_hidden_dim,kernel_size=(3, 3),bias=False)\n",
    "        self.decoder_3 = ConvLSTMCell(input_dim=num_hidden_dim, hidden_dim=num_hidden_dim,kernel_size=(3, 3),bias=False)\n",
    "        self.decoder_4 = ConvLSTMCell(input_dim=num_hidden_dim,hidden_dim=num_hidden_dim,kernel_size=(3, 3),bias=False)\n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=num_hidden_dim,out_channels=1,kernel_size=(1, 3, 3),padding=(0, 1, 1))\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, States):\n",
    "        \"\"\"\n",
    "        Build the encoder-decoder forward pass\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        x: \n",
    "            5-D Tensor of shape (b, t, c, h, w)       #   batch_size, time_step, num_channel, height, width\n",
    "        seq_len: int\n",
    "            Length of input sequence.\n",
    "        future_step: int\n",
    "            Length of predicted sequence.\n",
    "        States: bool\n",
    "            Initialized states for each cell.\n",
    "        \"\"\"\n",
    "        \n",
    "        # List to store the final hidden state of all the predicted frames \n",
    "        outputs = []\n",
    "        \n",
    "        # The initialized state tensors\n",
    "        h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4 = States \n",
    "        \n",
    "        \n",
    "        # encoder\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        for index in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1(input=x[:, index, :, :],cur_state=[h_t, c_t])  \n",
    "            h_t2, c_t2 = self.encoder_2(input=h_t,cur_state=[h_t2, c_t2])\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # encoded_vector: \"context\" from the encoder\n",
    "        encoded_vector = None\n",
    "        # YOUR CODE HERE\n",
    "        predicted_frames=[]\n",
    "        encoded_vector = h_t2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # decoder: you should store the final hidden states h_t4 from decoder in list outputs\n",
    "        # YOUR CODE HERE\n",
    "        i=0\n",
    "        while i < future_step:\n",
    "            h_t3, c_t3 = self.decoder_3(encoded_vector,\n",
    "                                                 [h_t3, c_t3])  \n",
    "            h_t4, c_t4 = self.decoder_4(h_t3,\n",
    "                                                 [h_t4, c_t4])  \n",
    "            encoded_vector = h_t4\n",
    "            predicted_frames += [h_t4]  \n",
    "            i += 1 \n",
    "        \n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # regression\n",
    "        predicted_frames = torch.stack(predicted_frames, 1)\n",
    "        predicted_frames = predicted_frames.permute(0, 2, 1, 3, 4)\n",
    "        predicted_frames = self.decoder_CNN(predicted_frames)\n",
    "        predicted_frames = torch.nn.Sigmoid()(predicted_frames)\n",
    "\n",
    "        return predicted_frames\n",
    "\n",
    "    def forward(self, x, future_seq=0):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, num_channel, height, width\n",
    "        future_seq:\n",
    "            Length of predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        \n",
    "        h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4 = [None for i in range(8)]\n",
    "        \n",
    "        # initialize hidden states\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        h_t, c_t = self.encoder_1.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t2, c_t2 = self.encoder_2.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t3, c_t3 = self.decoder_3.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t4, c_t4 = self.decoder_4.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        States = [h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4]\n",
    "        \n",
    "        predicted_frames = None\n",
    "        \n",
    "        # autoencoder forward\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        predicted_frames = self.autoencoder(x, seq_len, future_seq, States)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return predicted_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a29703",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7ffb3ab8b1646fd2ad753ffe3770f08",
     "grade": false,
     "grade_id": "cell-e03091d5aa5f2866",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Training and Evaluation\n",
    "\n",
    "We provide you helper functions for training and evaluation:\n",
    "\n",
    "```python\n",
    "    def save_img(img, args):\n",
    "        \"\"\"\n",
    "        Save images in your checkpoint directory\n",
    "        \"\"\"\n",
    "    \n",
    "    def show_video(x, y_hat, y):\n",
    "        \"\"\"\n",
    "        Make the predicted frames and ground truth frames as a image\n",
    "        \"\"\"\n",
    "\n",
    "    def train(net, loader, criterion, optimizer, epoch, args):\n",
    "        \"\"\"\n",
    "        Training function for each epoch\n",
    "        \"\"\"\n",
    "\n",
    "    def evaluate_epoch(net, loader, criterion, epoch, args):\n",
    "        \"\"\"\n",
    "        Evaluation function for each epoch\n",
    "        \"\"\"    \n",
    "        \n",
    "    def evaluate(net, loader, criterion, args):\n",
    "        \"\"\"\n",
    "        Evaluation function for the whole test set\n",
    "        \"\"\"\n",
    "\n",
    "    def create_optimizer(net, learning_rate):\n",
    "        \"\"\"\n",
    "        Create optimizer for updating network parameters\n",
    "        \"\"\"\n",
    "\n",
    "    def create_criterion():\n",
    "        \"\"\"\n",
    "        Create criterion for gradients backward\n",
    "        \"\"\"\n",
    "        \n",
    "    def checkpoint(net, epoch, cur_loss, args):\n",
    "        \"\"\"\n",
    "        Save your trained model as file\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "Here are some examples of output, you are expected to get similar results or even better! (Top row contains the input sequence and the predicted frames, while the second row represents the input sequence and ground truth)\n",
    "\n",
    "<img src=\"./images/result50.jpg\" width=\"90%\">\n",
    "<img src=\"./images/result55.jpg\" width=\"90%\">\n",
    "<img src=\"./images/result62.jpg\" width=\"90%\">\n",
    "\n",
    "The Average Mean-Square Error (MSE) on the test set of a model trained with 100 epochs is around **0.0280**. \n",
    "\n",
    "**Bonus (10%)**: Based on the above conclusion, you could think of ways to improve your code, and explain your observations and method in the report.\n",
    "\n",
    "---\n",
    "\n",
    "We use ``argparse`` to manage the experimental setting, e.g., hyperparameters and data directory. You can follow the default setting or change it with your choices.\n",
    "\n",
    "If \"out-of-memory\" error message occurs, you may try to reduce the batch size or hidden_dims. \n",
    "\n",
    "When the training is done, you can set ``args.load=True`` and ``args.eval=True`` to get both qualititative and quantitative analysis of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5a0dd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed6197c3243f1ac14d4d9072f2637c44",
     "grade": true,
     "grade_id": "cell-1b7a844fe3c818d1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/100], Step: [0] learning_rate: 0.0001, loss: 0.2389\n",
      "Epoch:[1/100], Step: [50] learning_rate: 0.0001, loss: 0.0464\n",
      "Epoch:[1/100], Step: [100] learning_rate: 0.0001, loss: 0.0379\n",
      "Epoch:[1/100], Step: [150] learning_rate: 0.0001, loss: 0.0407\n",
      "Epoch:[1/100], Step: [200] learning_rate: 0.0001, loss: 0.0429\n",
      "Epoch:[1/100], Step: [250] learning_rate: 0.0001, loss: 0.0366\n",
      "Epoch:[1/100], Step: [300] learning_rate: 0.0001, loss: 0.0384\n",
      "Epoch:[1/100], Step: [350] learning_rate: 0.0001, loss: 0.0413\n",
      "Epoch:[1/100], Step: [400] learning_rate: 0.0001, loss: 0.0365\n",
      "Epoch:[1/100], Step: [450] learning_rate: 0.0001, loss: 0.0397\n",
      "Epoch:[1/100], Step: [500] learning_rate: 0.0001, loss: 0.0377\n",
      "Epoch:[1/100], Step: [550] learning_rate: 0.0001, loss: 0.0374\n",
      "Epoch:[1/100], Step: [600] learning_rate: 0.0001, loss: 0.0387\n",
      "Average MSE loss on test dataset: 0.0400\n",
      "Saving checkpoints at 1 epochs.\n",
      "Epoch:[2/100], Step: [0] learning_rate: 0.0001, loss: 0.0394\n",
      "Epoch:[2/100], Step: [50] learning_rate: 0.0001, loss: 0.0385\n",
      "Epoch:[2/100], Step: [100] learning_rate: 0.0001, loss: 0.0368\n",
      "Epoch:[2/100], Step: [150] learning_rate: 0.0001, loss: 0.0417\n",
      "Epoch:[2/100], Step: [200] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[2/100], Step: [250] learning_rate: 0.0001, loss: 0.0379\n",
      "Epoch:[2/100], Step: [300] learning_rate: 0.0001, loss: 0.0365\n",
      "Epoch:[2/100], Step: [350] learning_rate: 0.0001, loss: 0.0396\n",
      "Epoch:[2/100], Step: [400] learning_rate: 0.0001, loss: 0.0381\n",
      "Epoch:[2/100], Step: [450] learning_rate: 0.0001, loss: 0.0397\n",
      "Epoch:[2/100], Step: [500] learning_rate: 0.0001, loss: 0.0419\n",
      "Epoch:[2/100], Step: [550] learning_rate: 0.0001, loss: 0.0384\n",
      "Epoch:[2/100], Step: [600] learning_rate: 0.0001, loss: 0.0400\n",
      "Average MSE loss on test dataset: 0.0394\n",
      "Saving checkpoints at 2 epochs.\n",
      "Epoch:[3/100], Step: [0] learning_rate: 0.0001, loss: 0.0385\n",
      "Epoch:[3/100], Step: [50] learning_rate: 0.0001, loss: 0.0345\n",
      "Epoch:[3/100], Step: [100] learning_rate: 0.0001, loss: 0.0409\n",
      "Epoch:[3/100], Step: [150] learning_rate: 0.0001, loss: 0.0411\n",
      "Epoch:[3/100], Step: [200] learning_rate: 0.0001, loss: 0.0415\n",
      "Epoch:[3/100], Step: [250] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[3/100], Step: [300] learning_rate: 0.0001, loss: 0.0403\n",
      "Epoch:[3/100], Step: [350] learning_rate: 0.0001, loss: 0.0387\n",
      "Epoch:[3/100], Step: [400] learning_rate: 0.0001, loss: 0.0411\n",
      "Epoch:[3/100], Step: [450] learning_rate: 0.0001, loss: 0.0367\n",
      "Epoch:[3/100], Step: [500] learning_rate: 0.0001, loss: 0.0358\n",
      "Epoch:[3/100], Step: [550] learning_rate: 0.0001, loss: 0.0375\n",
      "Epoch:[3/100], Step: [600] learning_rate: 0.0001, loss: 0.0370\n",
      "Average MSE loss on test dataset: 0.0391\n",
      "Saving checkpoints at 3 epochs.\n",
      "Epoch:[4/100], Step: [0] learning_rate: 0.0001, loss: 0.0370\n",
      "Epoch:[4/100], Step: [50] learning_rate: 0.0001, loss: 0.0396\n",
      "Epoch:[4/100], Step: [100] learning_rate: 0.0001, loss: 0.0390\n",
      "Epoch:[4/100], Step: [150] learning_rate: 0.0001, loss: 0.0361\n",
      "Epoch:[4/100], Step: [200] learning_rate: 0.0001, loss: 0.0395\n",
      "Epoch:[4/100], Step: [250] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[4/100], Step: [300] learning_rate: 0.0001, loss: 0.0429\n",
      "Epoch:[4/100], Step: [350] learning_rate: 0.0001, loss: 0.0397\n",
      "Epoch:[4/100], Step: [400] learning_rate: 0.0001, loss: 0.0361\n",
      "Epoch:[4/100], Step: [450] learning_rate: 0.0001, loss: 0.0368\n",
      "Epoch:[4/100], Step: [500] learning_rate: 0.0001, loss: 0.0381\n",
      "Epoch:[4/100], Step: [550] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[4/100], Step: [600] learning_rate: 0.0001, loss: 0.0404\n",
      "Average MSE loss on test dataset: 0.0390\n",
      "Saving checkpoints at 4 epochs.\n",
      "Epoch:[5/100], Step: [0] learning_rate: 0.0001, loss: 0.0388\n",
      "Epoch:[5/100], Step: [50] learning_rate: 0.0001, loss: 0.0384\n",
      "Epoch:[5/100], Step: [100] learning_rate: 0.0001, loss: 0.0421\n",
      "Epoch:[5/100], Step: [150] learning_rate: 0.0001, loss: 0.0370\n",
      "Epoch:[5/100], Step: [200] learning_rate: 0.0001, loss: 0.0367\n",
      "Epoch:[5/100], Step: [250] learning_rate: 0.0001, loss: 0.0420\n",
      "Epoch:[5/100], Step: [300] learning_rate: 0.0001, loss: 0.0372\n",
      "Epoch:[5/100], Step: [350] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[5/100], Step: [400] learning_rate: 0.0001, loss: 0.0386\n",
      "Epoch:[5/100], Step: [450] learning_rate: 0.0001, loss: 0.0390\n",
      "Epoch:[5/100], Step: [500] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[5/100], Step: [550] learning_rate: 0.0001, loss: 0.0416\n",
      "Epoch:[5/100], Step: [600] learning_rate: 0.0001, loss: 0.0372\n",
      "Average MSE loss on test dataset: 0.0392\n",
      "Saving checkpoints at 5 epochs.\n",
      "Epoch:[6/100], Step: [0] learning_rate: 0.0001, loss: 0.0385\n",
      "Epoch:[6/100], Step: [50] learning_rate: 0.0001, loss: 0.0375\n",
      "Epoch:[6/100], Step: [100] learning_rate: 0.0001, loss: 0.0371\n",
      "Epoch:[6/100], Step: [150] learning_rate: 0.0001, loss: 0.0390\n",
      "Epoch:[6/100], Step: [200] learning_rate: 0.0001, loss: 0.0364\n",
      "Epoch:[6/100], Step: [250] learning_rate: 0.0001, loss: 0.0378\n",
      "Epoch:[6/100], Step: [300] learning_rate: 0.0001, loss: 0.0423\n",
      "Epoch:[6/100], Step: [350] learning_rate: 0.0001, loss: 0.0415\n",
      "Epoch:[6/100], Step: [400] learning_rate: 0.0001, loss: 0.0425\n",
      "Epoch:[6/100], Step: [450] learning_rate: 0.0001, loss: 0.0374\n",
      "Epoch:[6/100], Step: [500] learning_rate: 0.0001, loss: 0.0412\n",
      "Epoch:[6/100], Step: [550] learning_rate: 0.0001, loss: 0.0380\n",
      "Epoch:[6/100], Step: [600] learning_rate: 0.0001, loss: 0.0350\n",
      "Average MSE loss on test dataset: 0.0389\n",
      "Saving checkpoints at 6 epochs.\n",
      "Epoch:[7/100], Step: [0] learning_rate: 0.0001, loss: 0.0393\n",
      "Epoch:[7/100], Step: [50] learning_rate: 0.0001, loss: 0.0405\n",
      "Epoch:[7/100], Step: [100] learning_rate: 0.0001, loss: 0.0365\n",
      "Epoch:[7/100], Step: [150] learning_rate: 0.0001, loss: 0.0368\n",
      "Epoch:[7/100], Step: [200] learning_rate: 0.0001, loss: 0.0379\n",
      "Epoch:[7/100], Step: [250] learning_rate: 0.0001, loss: 0.0391\n",
      "Epoch:[7/100], Step: [300] learning_rate: 0.0001, loss: 0.0386\n",
      "Epoch:[7/100], Step: [350] learning_rate: 0.0001, loss: 0.0382\n",
      "Epoch:[7/100], Step: [400] learning_rate: 0.0001, loss: 0.0412\n",
      "Epoch:[7/100], Step: [450] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[7/100], Step: [500] learning_rate: 0.0001, loss: 0.0373\n",
      "Epoch:[7/100], Step: [550] learning_rate: 0.0001, loss: 0.0359\n",
      "Epoch:[7/100], Step: [600] learning_rate: 0.0001, loss: 0.0385\n",
      "Average MSE loss on test dataset: 0.0363\n",
      "Saving checkpoints at 7 epochs.\n",
      "Epoch:[8/100], Step: [0] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[8/100], Step: [50] learning_rate: 0.0001, loss: 0.0365\n",
      "Epoch:[8/100], Step: [100] learning_rate: 0.0001, loss: 0.0370\n",
      "Epoch:[8/100], Step: [150] learning_rate: 0.0001, loss: 0.0374\n",
      "Epoch:[8/100], Step: [200] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[8/100], Step: [250] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[8/100], Step: [300] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[8/100], Step: [350] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[8/100], Step: [400] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[8/100], Step: [450] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[8/100], Step: [500] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[8/100], Step: [550] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[8/100], Step: [600] learning_rate: 0.0001, loss: 0.0349\n",
      "Average MSE loss on test dataset: 0.0351\n",
      "Saving checkpoints at 8 epochs.\n",
      "Epoch:[9/100], Step: [0] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[9/100], Step: [50] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[9/100], Step: [100] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[9/100], Step: [150] learning_rate: 0.0001, loss: 0.0363\n",
      "Epoch:[9/100], Step: [200] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[9/100], Step: [250] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[9/100], Step: [300] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[9/100], Step: [350] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[9/100], Step: [400] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[9/100], Step: [450] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[9/100], Step: [500] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[9/100], Step: [550] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[9/100], Step: [600] learning_rate: 0.0001, loss: 0.0350\n",
      "Average MSE loss on test dataset: 0.0345\n",
      "Saving checkpoints at 9 epochs.\n",
      "Epoch:[10/100], Step: [0] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[10/100], Step: [50] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[10/100], Step: [100] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[10/100], Step: [150] learning_rate: 0.0001, loss: 0.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[10/100], Step: [200] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[10/100], Step: [250] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[10/100], Step: [300] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[10/100], Step: [350] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[10/100], Step: [400] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[10/100], Step: [450] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[10/100], Step: [500] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[10/100], Step: [550] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[10/100], Step: [600] learning_rate: 0.0001, loss: 0.0343\n",
      "Average MSE loss on test dataset: 0.0340\n",
      "Saving checkpoints at 10 epochs.\n",
      "Epoch:[11/100], Step: [0] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[11/100], Step: [50] learning_rate: 0.0001, loss: 0.0359\n",
      "Epoch:[11/100], Step: [100] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[11/100], Step: [150] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[11/100], Step: [200] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[11/100], Step: [250] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[11/100], Step: [300] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[11/100], Step: [350] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[11/100], Step: [400] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[11/100], Step: [450] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[11/100], Step: [500] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[11/100], Step: [550] learning_rate: 0.0001, loss: 0.0364\n",
      "Epoch:[11/100], Step: [600] learning_rate: 0.0001, loss: 0.0316\n",
      "Average MSE loss on test dataset: 0.0340\n",
      "Saving checkpoints at 11 epochs.\n",
      "Epoch:[12/100], Step: [0] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[12/100], Step: [50] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[12/100], Step: [100] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[12/100], Step: [150] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[12/100], Step: [200] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[12/100], Step: [250] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[12/100], Step: [300] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[12/100], Step: [350] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[12/100], Step: [400] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[12/100], Step: [450] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[12/100], Step: [500] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[12/100], Step: [550] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[12/100], Step: [600] learning_rate: 0.0001, loss: 0.0316\n",
      "Average MSE loss on test dataset: 0.0335\n",
      "Saving checkpoints at 12 epochs.\n",
      "Epoch:[13/100], Step: [0] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[13/100], Step: [50] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[13/100], Step: [100] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[13/100], Step: [150] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[13/100], Step: [200] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[13/100], Step: [250] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[13/100], Step: [300] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[13/100], Step: [350] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[13/100], Step: [400] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[13/100], Step: [450] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[13/100], Step: [500] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[13/100], Step: [550] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[13/100], Step: [600] learning_rate: 0.0001, loss: 0.0309\n",
      "Average MSE loss on test dataset: 0.0331\n",
      "Saving checkpoints at 13 epochs.\n",
      "Epoch:[14/100], Step: [0] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[14/100], Step: [50] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[14/100], Step: [100] learning_rate: 0.0001, loss: 0.0365\n",
      "Epoch:[14/100], Step: [150] learning_rate: 0.0001, loss: 0.0358\n",
      "Epoch:[14/100], Step: [200] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[14/100], Step: [250] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[14/100], Step: [300] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[14/100], Step: [350] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[14/100], Step: [400] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[14/100], Step: [450] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[14/100], Step: [500] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[14/100], Step: [550] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[14/100], Step: [600] learning_rate: 0.0001, loss: 0.0289\n",
      "Average MSE loss on test dataset: 0.0330\n",
      "Saving checkpoints at 14 epochs.\n",
      "Epoch:[15/100], Step: [0] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[15/100], Step: [50] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[15/100], Step: [100] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[15/100], Step: [150] learning_rate: 0.0001, loss: 0.0353\n",
      "Epoch:[15/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[15/100], Step: [250] learning_rate: 0.0001, loss: 0.0340\n",
      "Epoch:[15/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[15/100], Step: [350] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[15/100], Step: [400] learning_rate: 0.0001, loss: 0.0359\n",
      "Epoch:[15/100], Step: [450] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[15/100], Step: [500] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[15/100], Step: [550] learning_rate: 0.0001, loss: 0.0344\n",
      "Epoch:[15/100], Step: [600] learning_rate: 0.0001, loss: 0.0306\n",
      "Average MSE loss on test dataset: 0.0326\n",
      "Saving checkpoints at 15 epochs.\n",
      "Epoch:[16/100], Step: [0] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[16/100], Step: [50] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[16/100], Step: [100] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[16/100], Step: [150] learning_rate: 0.0001, loss: 0.0350\n",
      "Epoch:[16/100], Step: [200] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[16/100], Step: [250] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[16/100], Step: [300] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[16/100], Step: [350] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[16/100], Step: [400] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[16/100], Step: [450] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[16/100], Step: [500] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[16/100], Step: [550] learning_rate: 0.0001, loss: 0.0347\n",
      "Epoch:[16/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0324\n",
      "Saving checkpoints at 16 epochs.\n",
      "Epoch:[17/100], Step: [0] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[17/100], Step: [50] learning_rate: 0.0001, loss: 0.0361\n",
      "Epoch:[17/100], Step: [100] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[17/100], Step: [150] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[17/100], Step: [200] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[17/100], Step: [250] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[17/100], Step: [300] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[17/100], Step: [350] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[17/100], Step: [400] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[17/100], Step: [450] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[17/100], Step: [500] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[17/100], Step: [550] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[17/100], Step: [600] learning_rate: 0.0001, loss: 0.0343\n",
      "Average MSE loss on test dataset: 0.0324\n",
      "Saving checkpoints at 17 epochs.\n",
      "Epoch:[18/100], Step: [0] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[18/100], Step: [50] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[18/100], Step: [100] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[18/100], Step: [150] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[18/100], Step: [200] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[18/100], Step: [250] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[18/100], Step: [300] learning_rate: 0.0001, loss: 0.0342\n",
      "Epoch:[18/100], Step: [350] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[18/100], Step: [400] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[18/100], Step: [450] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[18/100], Step: [500] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[18/100], Step: [550] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[18/100], Step: [600] learning_rate: 0.0001, loss: 0.0361\n",
      "Average MSE loss on test dataset: 0.0321\n",
      "Saving checkpoints at 18 epochs.\n",
      "Epoch:[19/100], Step: [0] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[19/100], Step: [50] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[19/100], Step: [100] learning_rate: 0.0001, loss: 0.0358\n",
      "Epoch:[19/100], Step: [150] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[19/100], Step: [200] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[19/100], Step: [250] learning_rate: 0.0001, loss: 0.0338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[19/100], Step: [300] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[19/100], Step: [350] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[19/100], Step: [400] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[19/100], Step: [450] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[19/100], Step: [500] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[19/100], Step: [550] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[19/100], Step: [600] learning_rate: 0.0001, loss: 0.0322\n",
      "Average MSE loss on test dataset: 0.0321\n",
      "Saving checkpoints at 19 epochs.\n",
      "Epoch:[20/100], Step: [0] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[20/100], Step: [50] learning_rate: 0.0001, loss: 0.0351\n",
      "Epoch:[20/100], Step: [100] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[20/100], Step: [150] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[20/100], Step: [200] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[20/100], Step: [250] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[20/100], Step: [300] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[20/100], Step: [350] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[20/100], Step: [400] learning_rate: 0.0001, loss: 0.0354\n",
      "Epoch:[20/100], Step: [450] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[20/100], Step: [500] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[20/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[20/100], Step: [600] learning_rate: 0.0001, loss: 0.0301\n",
      "Average MSE loss on test dataset: 0.0323\n",
      "Saving checkpoints at 20 epochs.\n",
      "Epoch:[21/100], Step: [0] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[21/100], Step: [50] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[21/100], Step: [100] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[21/100], Step: [150] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[21/100], Step: [200] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[21/100], Step: [250] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[21/100], Step: [300] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[21/100], Step: [350] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[21/100], Step: [400] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[21/100], Step: [450] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[21/100], Step: [500] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[21/100], Step: [550] learning_rate: 0.0001, loss: 0.0349\n",
      "Epoch:[21/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0319\n",
      "Saving checkpoints at 21 epochs.\n",
      "Epoch:[22/100], Step: [0] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[22/100], Step: [50] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[22/100], Step: [100] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[22/100], Step: [150] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[22/100], Step: [200] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[22/100], Step: [250] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[22/100], Step: [300] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[22/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[22/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[22/100], Step: [450] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[22/100], Step: [500] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[22/100], Step: [550] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[22/100], Step: [600] learning_rate: 0.0001, loss: 0.0317\n",
      "Average MSE loss on test dataset: 0.0319\n",
      "Saving checkpoints at 22 epochs.\n",
      "Epoch:[23/100], Step: [0] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[23/100], Step: [50] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[23/100], Step: [100] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[23/100], Step: [150] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[23/100], Step: [200] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[23/100], Step: [250] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[23/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[23/100], Step: [350] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[23/100], Step: [400] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[23/100], Step: [450] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[23/100], Step: [500] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[23/100], Step: [550] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[23/100], Step: [600] learning_rate: 0.0001, loss: 0.0309\n",
      "Average MSE loss on test dataset: 0.0319\n",
      "Saving checkpoints at 23 epochs.\n",
      "Epoch:[24/100], Step: [0] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[24/100], Step: [50] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[24/100], Step: [100] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[24/100], Step: [150] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[24/100], Step: [200] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[24/100], Step: [250] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[24/100], Step: [300] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[24/100], Step: [350] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[24/100], Step: [400] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[24/100], Step: [450] learning_rate: 0.0001, loss: 0.0352\n",
      "Epoch:[24/100], Step: [500] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[24/100], Step: [550] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[24/100], Step: [600] learning_rate: 0.0001, loss: 0.0266\n",
      "Average MSE loss on test dataset: 0.0320\n",
      "Saving checkpoints at 24 epochs.\n",
      "Epoch:[25/100], Step: [0] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[25/100], Step: [50] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[25/100], Step: [100] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[25/100], Step: [150] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[25/100], Step: [200] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[25/100], Step: [250] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[25/100], Step: [300] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[25/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[25/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[25/100], Step: [450] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[25/100], Step: [500] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[25/100], Step: [550] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[25/100], Step: [600] learning_rate: 0.0001, loss: 0.0307\n",
      "Average MSE loss on test dataset: 0.0317\n",
      "Saving checkpoints at 25 epochs.\n",
      "Epoch:[26/100], Step: [0] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[26/100], Step: [50] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[26/100], Step: [100] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[26/100], Step: [150] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[26/100], Step: [200] learning_rate: 0.0001, loss: 0.0343\n",
      "Epoch:[26/100], Step: [250] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[26/100], Step: [300] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[26/100], Step: [350] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[26/100], Step: [400] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[26/100], Step: [450] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[26/100], Step: [500] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[26/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[26/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0316\n",
      "Saving checkpoints at 26 epochs.\n",
      "Epoch:[27/100], Step: [0] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[27/100], Step: [50] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[27/100], Step: [100] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[27/100], Step: [150] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[27/100], Step: [200] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[27/100], Step: [250] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[27/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[27/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[27/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[27/100], Step: [450] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[27/100], Step: [500] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[27/100], Step: [550] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[27/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0316\n",
      "Saving checkpoints at 27 epochs.\n",
      "Epoch:[28/100], Step: [0] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[28/100], Step: [50] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[28/100], Step: [100] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[28/100], Step: [150] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[28/100], Step: [200] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[28/100], Step: [250] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[28/100], Step: [300] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[28/100], Step: [350] learning_rate: 0.0001, loss: 0.0315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[28/100], Step: [400] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[28/100], Step: [450] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[28/100], Step: [500] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[28/100], Step: [550] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[28/100], Step: [600] learning_rate: 0.0001, loss: 0.0326\n",
      "Average MSE loss on test dataset: 0.0316\n",
      "Saving checkpoints at 28 epochs.\n",
      "Epoch:[29/100], Step: [0] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[29/100], Step: [50] learning_rate: 0.0001, loss: 0.0356\n",
      "Epoch:[29/100], Step: [100] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[29/100], Step: [150] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[29/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[29/100], Step: [250] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[29/100], Step: [300] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[29/100], Step: [350] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[29/100], Step: [400] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[29/100], Step: [450] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[29/100], Step: [500] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[29/100], Step: [550] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[29/100], Step: [600] learning_rate: 0.0001, loss: 0.0315\n",
      "Average MSE loss on test dataset: 0.0315\n",
      "Saving checkpoints at 29 epochs.\n",
      "Epoch:[30/100], Step: [0] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[30/100], Step: [50] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[30/100], Step: [100] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[30/100], Step: [150] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[30/100], Step: [200] learning_rate: 0.0001, loss: 0.0345\n",
      "Epoch:[30/100], Step: [250] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[30/100], Step: [300] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[30/100], Step: [350] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[30/100], Step: [400] learning_rate: 0.0001, loss: 0.0339\n",
      "Epoch:[30/100], Step: [450] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[30/100], Step: [500] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[30/100], Step: [550] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[30/100], Step: [600] learning_rate: 0.0001, loss: 0.0315\n",
      "Average MSE loss on test dataset: 0.0314\n",
      "Saving checkpoints at 30 epochs.\n",
      "Epoch:[31/100], Step: [0] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[31/100], Step: [50] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[31/100], Step: [100] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[31/100], Step: [150] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[31/100], Step: [200] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[31/100], Step: [250] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[31/100], Step: [300] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[31/100], Step: [350] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[31/100], Step: [400] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[31/100], Step: [450] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[31/100], Step: [500] learning_rate: 0.0001, loss: 0.0328\n",
      "Epoch:[31/100], Step: [550] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[31/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0314\n",
      "Saving checkpoints at 31 epochs.\n",
      "Epoch:[32/100], Step: [0] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[32/100], Step: [50] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[32/100], Step: [100] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[32/100], Step: [150] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[32/100], Step: [200] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[32/100], Step: [250] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[32/100], Step: [300] learning_rate: 0.0001, loss: 0.0362\n",
      "Epoch:[32/100], Step: [350] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[32/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[32/100], Step: [450] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[32/100], Step: [500] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[32/100], Step: [550] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[32/100], Step: [600] learning_rate: 0.0001, loss: 0.0319\n",
      "Average MSE loss on test dataset: 0.0314\n",
      "Saving checkpoints at 32 epochs.\n",
      "Epoch:[33/100], Step: [0] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[33/100], Step: [50] learning_rate: 0.0001, loss: 0.0346\n",
      "Epoch:[33/100], Step: [100] learning_rate: 0.0001, loss: 0.0357\n",
      "Epoch:[33/100], Step: [150] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[33/100], Step: [200] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[33/100], Step: [250] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[33/100], Step: [300] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[33/100], Step: [350] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[33/100], Step: [400] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[33/100], Step: [450] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[33/100], Step: [500] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[33/100], Step: [550] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[33/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0313\n",
      "Saving checkpoints at 33 epochs.\n",
      "Epoch:[34/100], Step: [0] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[34/100], Step: [50] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[34/100], Step: [100] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[34/100], Step: [150] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[34/100], Step: [200] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[34/100], Step: [250] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[34/100], Step: [300] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[34/100], Step: [350] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[34/100], Step: [400] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[34/100], Step: [450] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[34/100], Step: [500] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[34/100], Step: [550] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[34/100], Step: [600] learning_rate: 0.0001, loss: 0.0313\n",
      "Average MSE loss on test dataset: 0.0312\n",
      "Saving checkpoints at 34 epochs.\n",
      "Epoch:[35/100], Step: [0] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[35/100], Step: [50] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[35/100], Step: [100] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[35/100], Step: [150] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[35/100], Step: [200] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[35/100], Step: [250] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[35/100], Step: [300] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[35/100], Step: [350] learning_rate: 0.0001, loss: 0.0356\n",
      "Epoch:[35/100], Step: [400] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[35/100], Step: [450] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[35/100], Step: [500] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[35/100], Step: [550] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[35/100], Step: [600] learning_rate: 0.0001, loss: 0.0301\n",
      "Average MSE loss on test dataset: 0.0312\n",
      "Saving checkpoints at 35 epochs.\n",
      "Epoch:[36/100], Step: [0] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[36/100], Step: [50] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[36/100], Step: [100] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[36/100], Step: [150] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[36/100], Step: [200] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[36/100], Step: [250] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[36/100], Step: [300] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[36/100], Step: [350] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[36/100], Step: [400] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[36/100], Step: [450] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[36/100], Step: [500] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[36/100], Step: [550] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[36/100], Step: [600] learning_rate: 0.0001, loss: 0.0269\n",
      "Average MSE loss on test dataset: 0.0310\n",
      "Saving checkpoints at 36 epochs.\n",
      "Epoch:[37/100], Step: [0] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[37/100], Step: [50] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[37/100], Step: [100] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[37/100], Step: [150] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[37/100], Step: [200] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[37/100], Step: [250] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[37/100], Step: [300] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[37/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[37/100], Step: [400] learning_rate: 0.0001, loss: 0.0332\n",
      "Epoch:[37/100], Step: [450] learning_rate: 0.0001, loss: 0.0321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[37/100], Step: [500] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[37/100], Step: [550] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[37/100], Step: [600] learning_rate: 0.0001, loss: 0.0296\n",
      "Average MSE loss on test dataset: 0.0311\n",
      "Saving checkpoints at 37 epochs.\n",
      "Epoch:[38/100], Step: [0] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[38/100], Step: [50] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[38/100], Step: [100] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[38/100], Step: [150] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[38/100], Step: [200] learning_rate: 0.0001, loss: 0.0329\n",
      "Epoch:[38/100], Step: [250] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[38/100], Step: [300] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[38/100], Step: [350] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[38/100], Step: [400] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[38/100], Step: [450] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[38/100], Step: [500] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[38/100], Step: [550] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[38/100], Step: [600] learning_rate: 0.0001, loss: 0.0306\n",
      "Average MSE loss on test dataset: 0.0309\n",
      "Saving checkpoints at 38 epochs.\n",
      "Epoch:[39/100], Step: [0] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[39/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[39/100], Step: [100] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[39/100], Step: [150] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[39/100], Step: [200] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[39/100], Step: [250] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[39/100], Step: [300] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[39/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[39/100], Step: [400] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[39/100], Step: [450] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[39/100], Step: [500] learning_rate: 0.0001, loss: 0.0336\n",
      "Epoch:[39/100], Step: [550] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[39/100], Step: [600] learning_rate: 0.0001, loss: 0.0305\n",
      "Average MSE loss on test dataset: 0.0308\n",
      "Saving checkpoints at 39 epochs.\n",
      "Epoch:[40/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[40/100], Step: [50] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[40/100], Step: [100] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[40/100], Step: [150] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[40/100], Step: [200] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[40/100], Step: [250] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[40/100], Step: [300] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[40/100], Step: [350] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[40/100], Step: [400] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[40/100], Step: [450] learning_rate: 0.0001, loss: 0.0338\n",
      "Epoch:[40/100], Step: [500] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[40/100], Step: [550] learning_rate: 0.0001, loss: 0.0333\n",
      "Epoch:[40/100], Step: [600] learning_rate: 0.0001, loss: 0.0326\n",
      "Average MSE loss on test dataset: 0.0309\n",
      "Saving checkpoints at 40 epochs.\n",
      "Epoch:[41/100], Step: [0] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[41/100], Step: [50] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[41/100], Step: [100] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[41/100], Step: [150] learning_rate: 0.0001, loss: 0.0334\n",
      "Epoch:[41/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[41/100], Step: [250] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[41/100], Step: [300] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[41/100], Step: [350] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[41/100], Step: [400] learning_rate: 0.0001, loss: 0.0341\n",
      "Epoch:[41/100], Step: [450] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[41/100], Step: [500] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[41/100], Step: [550] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[41/100], Step: [600] learning_rate: 0.0001, loss: 0.0301\n",
      "Average MSE loss on test dataset: 0.0308\n",
      "Saving checkpoints at 41 epochs.\n",
      "Epoch:[42/100], Step: [0] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[42/100], Step: [50] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[42/100], Step: [100] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[42/100], Step: [150] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[42/100], Step: [200] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[42/100], Step: [250] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[42/100], Step: [300] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[42/100], Step: [350] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[42/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[42/100], Step: [450] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[42/100], Step: [500] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[42/100], Step: [550] learning_rate: 0.0001, loss: 0.0321\n",
      "Epoch:[42/100], Step: [600] learning_rate: 0.0001, loss: 0.0337\n",
      "Average MSE loss on test dataset: 0.0306\n",
      "Saving checkpoints at 42 epochs.\n",
      "Epoch:[43/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[43/100], Step: [50] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[43/100], Step: [100] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[43/100], Step: [150] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[43/100], Step: [200] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[43/100], Step: [250] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[43/100], Step: [300] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[43/100], Step: [350] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[43/100], Step: [400] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[43/100], Step: [450] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[43/100], Step: [500] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[43/100], Step: [550] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[43/100], Step: [600] learning_rate: 0.0001, loss: 0.0286\n",
      "Average MSE loss on test dataset: 0.0306\n",
      "Saving checkpoints at 43 epochs.\n",
      "Epoch:[44/100], Step: [0] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[44/100], Step: [50] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[44/100], Step: [100] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[44/100], Step: [150] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[44/100], Step: [200] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[44/100], Step: [250] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[44/100], Step: [300] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[44/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[44/100], Step: [400] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[44/100], Step: [450] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[44/100], Step: [500] learning_rate: 0.0001, loss: 0.0277\n",
      "Epoch:[44/100], Step: [550] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[44/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0306\n",
      "Saving checkpoints at 44 epochs.\n",
      "Epoch:[45/100], Step: [0] learning_rate: 0.0001, loss: 0.0348\n",
      "Epoch:[45/100], Step: [50] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[45/100], Step: [100] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[45/100], Step: [150] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[45/100], Step: [200] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[45/100], Step: [250] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[45/100], Step: [300] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[45/100], Step: [350] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[45/100], Step: [400] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[45/100], Step: [450] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[45/100], Step: [500] learning_rate: 0.0001, loss: 0.0361\n",
      "Epoch:[45/100], Step: [550] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[45/100], Step: [600] learning_rate: 0.0001, loss: 0.0303\n",
      "Average MSE loss on test dataset: 0.0305\n",
      "Saving checkpoints at 45 epochs.\n",
      "Epoch:[46/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[46/100], Step: [50] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[46/100], Step: [100] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[46/100], Step: [150] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[46/100], Step: [200] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[46/100], Step: [250] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[46/100], Step: [300] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[46/100], Step: [350] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[46/100], Step: [400] learning_rate: 0.0001, loss: 0.0335\n",
      "Epoch:[46/100], Step: [450] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[46/100], Step: [500] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[46/100], Step: [550] learning_rate: 0.0001, loss: 0.0296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[46/100], Step: [600] learning_rate: 0.0001, loss: 0.0322\n",
      "Average MSE loss on test dataset: 0.0305\n",
      "Saving checkpoints at 46 epochs.\n",
      "Epoch:[47/100], Step: [0] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[47/100], Step: [50] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[47/100], Step: [100] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[47/100], Step: [150] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[47/100], Step: [200] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[47/100], Step: [250] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[47/100], Step: [300] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[47/100], Step: [350] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[47/100], Step: [400] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[47/100], Step: [450] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[47/100], Step: [500] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[47/100], Step: [550] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[47/100], Step: [600] learning_rate: 0.0001, loss: 0.0305\n",
      "Average MSE loss on test dataset: 0.0304\n",
      "Saving checkpoints at 47 epochs.\n",
      "Epoch:[48/100], Step: [0] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[48/100], Step: [50] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[48/100], Step: [100] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[48/100], Step: [150] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[48/100], Step: [200] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[48/100], Step: [250] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[48/100], Step: [300] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[48/100], Step: [350] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[48/100], Step: [400] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[48/100], Step: [450] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[48/100], Step: [500] learning_rate: 0.0001, loss: 0.0331\n",
      "Epoch:[48/100], Step: [550] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[48/100], Step: [600] learning_rate: 0.0001, loss: 0.0287\n",
      "Average MSE loss on test dataset: 0.0302\n",
      "Saving checkpoints at 48 epochs.\n",
      "Epoch:[49/100], Step: [0] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[49/100], Step: [50] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[49/100], Step: [100] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[49/100], Step: [150] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[49/100], Step: [200] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[49/100], Step: [250] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[49/100], Step: [300] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[49/100], Step: [350] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[49/100], Step: [400] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[49/100], Step: [450] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[49/100], Step: [500] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[49/100], Step: [550] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[49/100], Step: [600] learning_rate: 0.0001, loss: 0.0316\n",
      "Average MSE loss on test dataset: 0.0302\n",
      "Saving checkpoints at 49 epochs.\n",
      "Epoch:[50/100], Step: [0] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[50/100], Step: [50] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[50/100], Step: [100] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[50/100], Step: [150] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[50/100], Step: [200] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[50/100], Step: [250] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[50/100], Step: [300] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[50/100], Step: [350] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[50/100], Step: [400] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[50/100], Step: [450] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[50/100], Step: [500] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[50/100], Step: [550] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[50/100], Step: [600] learning_rate: 0.0001, loss: 0.0280\n",
      "Average MSE loss on test dataset: 0.0301\n",
      "Saving checkpoints at 50 epochs.\n",
      "Epoch:[51/100], Step: [0] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[51/100], Step: [50] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[51/100], Step: [100] learning_rate: 0.0001, loss: 0.0266\n",
      "Epoch:[51/100], Step: [150] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[51/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[51/100], Step: [250] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[51/100], Step: [300] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[51/100], Step: [350] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[51/100], Step: [400] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[51/100], Step: [450] learning_rate: 0.0001, loss: 0.0324\n",
      "Epoch:[51/100], Step: [500] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[51/100], Step: [550] learning_rate: 0.0001, loss: 0.0252\n",
      "Epoch:[51/100], Step: [600] learning_rate: 0.0001, loss: 0.0325\n",
      "Average MSE loss on test dataset: 0.0301\n",
      "Saving checkpoints at 51 epochs.\n",
      "Epoch:[52/100], Step: [0] learning_rate: 0.0001, loss: 0.0326\n",
      "Epoch:[52/100], Step: [50] learning_rate: 0.0001, loss: 0.0308\n",
      "Epoch:[52/100], Step: [100] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[52/100], Step: [150] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[52/100], Step: [200] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[52/100], Step: [250] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[52/100], Step: [300] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[52/100], Step: [350] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[52/100], Step: [400] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[52/100], Step: [450] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[52/100], Step: [500] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[52/100], Step: [550] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[52/100], Step: [600] learning_rate: 0.0001, loss: 0.0300\n",
      "Average MSE loss on test dataset: 0.0300\n",
      "Saving checkpoints at 52 epochs.\n",
      "Epoch:[53/100], Step: [0] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[53/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[53/100], Step: [100] learning_rate: 0.0001, loss: 0.0253\n",
      "Epoch:[53/100], Step: [150] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[53/100], Step: [200] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[53/100], Step: [250] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[53/100], Step: [300] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[53/100], Step: [350] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[53/100], Step: [400] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[53/100], Step: [450] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[53/100], Step: [500] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[53/100], Step: [550] learning_rate: 0.0001, loss: 0.0307\n",
      "Epoch:[53/100], Step: [600] learning_rate: 0.0001, loss: 0.0308\n",
      "Average MSE loss on test dataset: 0.0299\n",
      "Saving checkpoints at 53 epochs.\n",
      "Epoch:[54/100], Step: [0] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[54/100], Step: [50] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[54/100], Step: [100] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[54/100], Step: [150] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[54/100], Step: [200] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[54/100], Step: [250] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[54/100], Step: [300] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[54/100], Step: [350] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[54/100], Step: [400] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[54/100], Step: [450] learning_rate: 0.0001, loss: 0.0278\n",
      "Epoch:[54/100], Step: [500] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[54/100], Step: [550] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[54/100], Step: [600] learning_rate: 0.0001, loss: 0.0304\n",
      "Average MSE loss on test dataset: 0.0299\n",
      "Saving checkpoints at 54 epochs.\n",
      "Epoch:[55/100], Step: [0] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[55/100], Step: [50] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[55/100], Step: [100] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[55/100], Step: [150] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[55/100], Step: [200] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[55/100], Step: [250] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[55/100], Step: [300] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[55/100], Step: [350] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[55/100], Step: [400] learning_rate: 0.0001, loss: 0.0330\n",
      "Epoch:[55/100], Step: [450] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[55/100], Step: [500] learning_rate: 0.0001, loss: 0.0320\n",
      "Epoch:[55/100], Step: [550] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[55/100], Step: [600] learning_rate: 0.0001, loss: 0.0298\n",
      "Average MSE loss on test dataset: 0.0299\n",
      "Saving checkpoints at 55 epochs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[56/100], Step: [0] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[56/100], Step: [50] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[56/100], Step: [100] learning_rate: 0.0001, loss: 0.0337\n",
      "Epoch:[56/100], Step: [150] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[56/100], Step: [200] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[56/100], Step: [250] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[56/100], Step: [300] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[56/100], Step: [350] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[56/100], Step: [400] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[56/100], Step: [450] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[56/100], Step: [500] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[56/100], Step: [550] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[56/100], Step: [600] learning_rate: 0.0001, loss: 0.0307\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 56 epochs.\n",
      "Epoch:[57/100], Step: [0] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[57/100], Step: [50] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[57/100], Step: [100] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[57/100], Step: [150] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[57/100], Step: [200] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[57/100], Step: [250] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[57/100], Step: [300] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[57/100], Step: [350] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[57/100], Step: [400] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[57/100], Step: [450] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[57/100], Step: [500] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[57/100], Step: [550] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[57/100], Step: [600] learning_rate: 0.0001, loss: 0.0283\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 57 epochs.\n",
      "Epoch:[58/100], Step: [0] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[58/100], Step: [50] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[58/100], Step: [100] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[58/100], Step: [150] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[58/100], Step: [200] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[58/100], Step: [250] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[58/100], Step: [300] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[58/100], Step: [350] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[58/100], Step: [400] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[58/100], Step: [450] learning_rate: 0.0001, loss: 0.0267\n",
      "Epoch:[58/100], Step: [500] learning_rate: 0.0001, loss: 0.0289\n",
      "Epoch:[58/100], Step: [550] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[58/100], Step: [600] learning_rate: 0.0001, loss: 0.0280\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 58 epochs.\n",
      "Epoch:[59/100], Step: [0] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[59/100], Step: [50] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[59/100], Step: [100] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[59/100], Step: [150] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[59/100], Step: [200] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[59/100], Step: [250] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[59/100], Step: [300] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[59/100], Step: [350] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[59/100], Step: [400] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[59/100], Step: [450] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[59/100], Step: [500] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[59/100], Step: [550] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[59/100], Step: [600] learning_rate: 0.0001, loss: 0.0276\n",
      "Average MSE loss on test dataset: 0.0297\n",
      "Saving checkpoints at 59 epochs.\n",
      "Epoch:[60/100], Step: [0] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[60/100], Step: [50] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[60/100], Step: [100] learning_rate: 0.0001, loss: 0.0319\n",
      "Epoch:[60/100], Step: [150] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[60/100], Step: [200] learning_rate: 0.0001, loss: 0.0251\n",
      "Epoch:[60/100], Step: [250] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[60/100], Step: [300] learning_rate: 0.0001, loss: 0.0306\n",
      "Epoch:[60/100], Step: [350] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[60/100], Step: [400] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[60/100], Step: [450] learning_rate: 0.0001, loss: 0.0288\n",
      "Epoch:[60/100], Step: [500] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[60/100], Step: [550] learning_rate: 0.0001, loss: 0.0312\n",
      "Epoch:[60/100], Step: [600] learning_rate: 0.0001, loss: 0.0298\n",
      "Average MSE loss on test dataset: 0.0295\n",
      "Saving checkpoints at 60 epochs.\n",
      "Epoch:[61/100], Step: [0] learning_rate: 0.0001, loss: 0.0276\n",
      "Epoch:[61/100], Step: [50] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[61/100], Step: [100] learning_rate: 0.0001, loss: 0.0286\n",
      "Epoch:[61/100], Step: [150] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[61/100], Step: [200] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[61/100], Step: [250] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[61/100], Step: [300] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[61/100], Step: [350] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[61/100], Step: [400] learning_rate: 0.0001, loss: 0.0323\n",
      "Epoch:[61/100], Step: [450] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[61/100], Step: [500] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[61/100], Step: [550] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[61/100], Step: [600] learning_rate: 0.0001, loss: 0.0289\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 61 epochs.\n",
      "Epoch:[62/100], Step: [0] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[62/100], Step: [50] learning_rate: 0.0001, loss: 0.0273\n",
      "Epoch:[62/100], Step: [100] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[62/100], Step: [150] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[62/100], Step: [200] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[62/100], Step: [250] learning_rate: 0.0001, loss: 0.0303\n",
      "Epoch:[62/100], Step: [300] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[62/100], Step: [350] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[62/100], Step: [400] learning_rate: 0.0001, loss: 0.0318\n",
      "Epoch:[62/100], Step: [450] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[62/100], Step: [500] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[62/100], Step: [550] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[62/100], Step: [600] learning_rate: 0.0001, loss: 0.0339\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 62 epochs.\n",
      "Epoch:[63/100], Step: [0] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[63/100], Step: [50] learning_rate: 0.0001, loss: 0.0298\n",
      "Epoch:[63/100], Step: [100] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[63/100], Step: [150] learning_rate: 0.0001, loss: 0.0311\n",
      "Epoch:[63/100], Step: [200] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[63/100], Step: [250] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[63/100], Step: [300] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[63/100], Step: [350] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[63/100], Step: [400] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[63/100], Step: [450] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[63/100], Step: [500] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[63/100], Step: [550] learning_rate: 0.0001, loss: 0.0296\n",
      "Epoch:[63/100], Step: [600] learning_rate: 0.0001, loss: 0.0292\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 63 epochs.\n",
      "Epoch:[64/100], Step: [0] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[64/100], Step: [50] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[64/100], Step: [100] learning_rate: 0.0001, loss: 0.0327\n",
      "Epoch:[64/100], Step: [150] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[64/100], Step: [200] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[64/100], Step: [250] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[64/100], Step: [300] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[64/100], Step: [350] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[64/100], Step: [400] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[64/100], Step: [450] learning_rate: 0.0001, loss: 0.0281\n",
      "Epoch:[64/100], Step: [500] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[64/100], Step: [550] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[64/100], Step: [600] learning_rate: 0.0001, loss: 0.0309\n",
      "Average MSE loss on test dataset: 0.0294\n",
      "Saving checkpoints at 64 epochs.\n",
      "Epoch:[65/100], Step: [0] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[65/100], Step: [50] learning_rate: 0.0001, loss: 0.0292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[65/100], Step: [100] learning_rate: 0.0001, loss: 0.0310\n",
      "Epoch:[65/100], Step: [150] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[65/100], Step: [200] learning_rate: 0.0001, loss: 0.0322\n",
      "Epoch:[65/100], Step: [250] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[65/100], Step: [300] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[65/100], Step: [350] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[65/100], Step: [400] learning_rate: 0.0001, loss: 0.0270\n",
      "Epoch:[65/100], Step: [450] learning_rate: 0.0001, loss: 0.0271\n",
      "Epoch:[65/100], Step: [500] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[65/100], Step: [550] learning_rate: 0.0001, loss: 0.0279\n",
      "Epoch:[65/100], Step: [600] learning_rate: 0.0001, loss: 0.0279\n",
      "Average MSE loss on test dataset: 0.0292\n",
      "Saving checkpoints at 65 epochs.\n",
      "Epoch:[66/100], Step: [0] learning_rate: 0.0001, loss: 0.0316\n",
      "Epoch:[66/100], Step: [50] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[66/100], Step: [100] learning_rate: 0.0001, loss: 0.0292\n",
      "Epoch:[66/100], Step: [150] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[66/100], Step: [200] learning_rate: 0.0001, loss: 0.0314\n",
      "Epoch:[66/100], Step: [250] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[66/100], Step: [300] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[66/100], Step: [350] learning_rate: 0.0001, loss: 0.0299\n",
      "Epoch:[66/100], Step: [400] learning_rate: 0.0001, loss: 0.0304\n",
      "Epoch:[66/100], Step: [450] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[66/100], Step: [500] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[66/100], Step: [550] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[66/100], Step: [600] learning_rate: 0.0001, loss: 0.0265\n",
      "Average MSE loss on test dataset: 0.0296\n",
      "Saving checkpoints at 66 epochs.\n",
      "Epoch:[67/100], Step: [0] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[67/100], Step: [50] learning_rate: 0.0001, loss: 0.0275\n",
      "Epoch:[67/100], Step: [100] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[67/100], Step: [150] learning_rate: 0.0001, loss: 0.0280\n",
      "Epoch:[67/100], Step: [200] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[67/100], Step: [250] learning_rate: 0.0001, loss: 0.0302\n",
      "Epoch:[67/100], Step: [300] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[67/100], Step: [350] learning_rate: 0.0001, loss: 0.0274\n",
      "Epoch:[67/100], Step: [400] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[67/100], Step: [450] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[67/100], Step: [500] learning_rate: 0.0001, loss: 0.0282\n",
      "Epoch:[67/100], Step: [550] learning_rate: 0.0001, loss: 0.0268\n",
      "Epoch:[67/100], Step: [600] learning_rate: 0.0001, loss: 0.0284\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 67 epochs.\n",
      "Epoch:[68/100], Step: [0] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[68/100], Step: [50] learning_rate: 0.0001, loss: 0.0295\n",
      "Epoch:[68/100], Step: [100] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[68/100], Step: [150] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[68/100], Step: [200] learning_rate: 0.0001, loss: 0.0305\n",
      "Epoch:[68/100], Step: [250] learning_rate: 0.0001, loss: 0.0301\n",
      "Epoch:[68/100], Step: [300] learning_rate: 0.0001, loss: 0.0309\n",
      "Epoch:[68/100], Step: [350] learning_rate: 0.0001, loss: 0.0254\n",
      "Epoch:[68/100], Step: [400] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[68/100], Step: [450] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[68/100], Step: [500] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[68/100], Step: [550] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[68/100], Step: [600] learning_rate: 0.0001, loss: 0.0289\n",
      "Average MSE loss on test dataset: 0.0292\n",
      "Saving checkpoints at 68 epochs.\n",
      "Epoch:[69/100], Step: [0] learning_rate: 0.0001, loss: 0.0287\n",
      "Epoch:[69/100], Step: [50] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[69/100], Step: [100] learning_rate: 0.0001, loss: 0.0261\n",
      "Epoch:[69/100], Step: [150] learning_rate: 0.0001, loss: 0.0284\n",
      "Epoch:[69/100], Step: [200] learning_rate: 0.0001, loss: 0.0263\n",
      "Epoch:[69/100], Step: [250] learning_rate: 0.0001, loss: 0.0313\n",
      "Epoch:[69/100], Step: [300] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[69/100], Step: [350] learning_rate: 0.0001, loss: 0.0260\n",
      "Epoch:[69/100], Step: [400] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[69/100], Step: [450] learning_rate: 0.0001, loss: 0.0297\n",
      "Epoch:[69/100], Step: [500] learning_rate: 0.0001, loss: 0.0264\n",
      "Epoch:[69/100], Step: [550] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[69/100], Step: [600] learning_rate: 0.0001, loss: 0.0279\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 69 epochs.\n",
      "Epoch:[70/100], Step: [0] learning_rate: 0.0001, loss: 0.0317\n",
      "Epoch:[70/100], Step: [50] learning_rate: 0.0001, loss: 0.0300\n",
      "Epoch:[70/100], Step: [100] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[70/100], Step: [150] learning_rate: 0.0001, loss: 0.0283\n",
      "Epoch:[70/100], Step: [200] learning_rate: 0.0001, loss: 0.0315\n",
      "Epoch:[70/100], Step: [250] learning_rate: 0.0001, loss: 0.0285\n",
      "Epoch:[70/100], Step: [300] learning_rate: 0.0001, loss: 0.0291\n",
      "Epoch:[70/100], Step: [350] learning_rate: 0.0001, loss: 0.0293\n",
      "Epoch:[70/100], Step: [400] learning_rate: 0.0001, loss: 0.0325\n",
      "Epoch:[70/100], Step: [450] learning_rate: 0.0001, loss: 0.0269\n",
      "Epoch:[70/100], Step: [500] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[70/100], Step: [550] learning_rate: 0.0001, loss: 0.0290\n",
      "Epoch:[70/100], Step: [600] learning_rate: 0.0001, loss: 0.0305\n",
      "Average MSE loss on test dataset: 0.0291\n",
      "Saving checkpoints at 70 epochs.\n",
      "Epoch:[71/100], Step: [0] learning_rate: 0.0001, loss: 0.0294\n",
      "Epoch:[71/100], Step: [50] learning_rate: 0.0001, loss: 0.0272\n",
      "Epoch:[71/100], Step: [100] learning_rate: 0.0001, loss: 0.0311\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5b54b5a2d9f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5b54b5a2d9f7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# traning function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Evaluation and visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoover/u7/bsaxena/project2_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, loader, criterion, optimizer, epoch, args)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    net = EncoderDecoderConvLSTM(num_hidden_dim=args.n_hidden_dim, in_channel=1)\n",
    "    criterion = create_criterion()\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                     dataset=train_set,\n",
    "                     batch_size=args.batch_size,\n",
    "                     shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=test_set,\n",
    "                    batch_size=args.batch_size,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    optimizer = create_optimizer(net, args.lr)\n",
    "    \n",
    "    # send net to gpu\n",
    "    net.to(args.device)\n",
    "    \n",
    "    if args.load:\n",
    "        net.load_state_dict(torch.load(os.path.join(args.ckpt_dir, 'net_best.pth')))\n",
    "        print(\"========== load checkpoints successfully! ==========\")\n",
    "    \n",
    "    if args.eval:\n",
    "        print(\"========== evaluating... ==========\")\n",
    "        evaluate(net, test_loader, criterion, args)\n",
    "        print(\"========== evaluation done! ==========\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # traning function\n",
    "        train(net, train_loader, criterion, optimizer, epoch, args)\n",
    "        \n",
    "        # Evaluation and visualization\n",
    "        if epoch % args.eval_epoch == 0:\n",
    "            cur_loss = evaluate_epoch(net, test_loader, criterion, epoch, args)\n",
    "            # checkpointing\n",
    "            checkpoint(net, epoch, cur_loss ,args)\n",
    "\n",
    "    print('Training Done!')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train for')\n",
    "    parser.add_argument('--eval_epoch', type=int, default=1, help='number of epochs to evaluate once')\n",
    "    parser.add_argument('--n_hidden_dim', type=int, default=64, help='number of hidden dim for ConvLSTM cells')\n",
    "    parser.add_argument('--n_steps_ahead', type=int, default=10, help='length of predicted sequences')\n",
    "    parser.add_argument('--ckpt_dir', type=str, default='./ckpt', help='where you save trained model')\n",
    "    parser.add_argument('--load', type=bool, default=False, help='whether to load the previsous checkpoint')\n",
    "    parser.add_argument('--eval', type=bool, default=False, help='whether to evaluate the whole test set without training')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if not os.path.exists(args.ckpt_dir):\n",
    "        os.makedirs(args.ckpt_dir)\n",
    "        \n",
    "    args.best_loss = float('inf')\n",
    "\n",
    "    # fix the random seed\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6ba1f",
   "metadata": {},
   "source": [
    "# **Writing questions for the report:**\n",
    "\n",
    "The following five questions are required to complete. \n",
    "\n",
    "Please create a new file called ``writeup.txt`` to include your answer.\n",
    "\n",
    "1. What is used as the context, or the input to the first decoder?\n",
    "2. Can this framework accept variable length of input video frames? Assuming the number of input frame is 10, can we possibly predict more than 10 output frames? If no, why not? If yes, please tell us how.\n",
    "3. Why do we use the sigmoid layer in forget gate? What ability does the sigmoid layer carry in this case?\n",
    "4. Which of the following architecture better capture spatio-temporal correlations: fully connected LSTM vs. ConvLSTM? What component is the inferior architecture lacking in comparison with the superior one?\n",
    "5. By examining visualizations, you will find that the predictions are fading away through time. Please write a paragraph in the report to explain why the generated frames are \"fading away\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac864275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396d458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439732e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c40f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b063fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b57c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c235c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
